{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9623e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk.data\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f4309c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"sample_preprocessed\"\n",
    "output_path = \"sample_summary\"\n",
    "dict_path = \"dictionary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3254204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec = {}\n",
    "doc_w_vec = {}\n",
    "total_docs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "226ab760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_df():\n",
    "    global df_vec\n",
    "\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    path = input_path\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            f_path = os.path.join(path, f)\n",
    "            with open(f_path, 'r', encoding='utf-8') as fp:\n",
    "                data = fp.read()\n",
    "            \n",
    "            sntncs = tokenizer.tokenize(data)\n",
    "            nor_stp_lmt = []\n",
    "\n",
    "            for s in sntncs:\n",
    "                s_nor_stp_lmt = \"\"\n",
    "                s = s.lower()\n",
    "                words = word_tokenize(s)\n",
    "                for w in words:\n",
    "                    if w not in stop:\n",
    "                        w = wordnet_lemmatizer.lemmatize(w)\n",
    "                        s_nor_stp_lmt += w + \" \"\n",
    "                nor_stp_lmt.append(s_nor_stp_lmt.strip())\n",
    "\n",
    "            # Build unique word set per document\n",
    "            unq_words = set()\n",
    "            for s in nor_stp_lmt:\n",
    "                for w in word_tokenize(s):\n",
    "                    if w != \".\":\n",
    "                        unq_words.add(w)\n",
    "\n",
    "            # Update DF vector\n",
    "            for k in unq_words:\n",
    "                df_vec[k] = df_vec.get(k, 0) + 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {f}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3f4ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_total_doc():\n",
    "    global total_docs\n",
    "    path = input_path\n",
    "    files = os.listdir(path)\n",
    "    total_docs = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84b9fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "     chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "     continuous_chunk = []\n",
    "     current_chunk = []\n",
    "     for i in chunked:\n",
    "         if type(i) == Tree:\n",
    "             current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "         elif current_chunk:\n",
    "             named_entity = \" \".join(current_chunk)\n",
    "             if named_entity not in continuous_chunk:\n",
    "                     continuous_chunk.append(named_entity)\n",
    "                     current_chunk = []\n",
    "             else:\n",
    "                 continue\n",
    "     return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46042a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b66a7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_legal_dict():\n",
    "    l_f = open(dict_path, \"r\")\n",
    "    for wd in l_f:\n",
    "        legal_words.append(wd)\n",
    "    l_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bf73251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tf_Idf():\n",
    "    global legal_words\n",
    "    global total_docs\n",
    "    global doc_w_vec\n",
    "    global df_vec\n",
    "\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    path = input_path\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            tf_idf_sntnc = {}\n",
    "            f_path = os.path.join(path, f)\n",
    "            \n",
    "            with open(f_path, 'r', encoding='utf-8') as fp:\n",
    "                data = fp.read()\n",
    "            \n",
    "            sntncs = tokenizer.tokenize(data)\n",
    "            norm_sents = []\n",
    "\n",
    "            for s in sntncs:\n",
    "                s_u = s.lower()\n",
    "                words = word_tokenize(s_u)\n",
    "                norm_sents.append(\" \".join(words))\n",
    "\n",
    "            # Compute TF\n",
    "            tf_vec = {}\n",
    "            length = 0\n",
    "            for s in norm_sents:\n",
    "                for w in word_tokenize(s):\n",
    "                    if w != \".\":\n",
    "                        length += 1\n",
    "                        tf_vec[w] = tf_vec.get(w, 0) + 1\n",
    "\n",
    "            tf_idf_doc = {}\n",
    "            for k in tf_vec:\n",
    "                tf_vec[k] = float(tf_vec[k]) / float(length)\n",
    "                tf_idf_doc[k] = tf_vec[k] * math.log10(float(total_docs) / float(df_vec.get(k, 1)))\n",
    "\n",
    "            doc_w_vec[f] = tf_idf_doc\n",
    "\n",
    "            std_list = []\n",
    "            for i in range(len(norm_sents)):\n",
    "                s = norm_sents[i]\n",
    "                ac_s = sntncs[i]\n",
    "                sm = sum(tf_idf_doc.get(w, 0) for w in word_tokenize(s))\n",
    "                no_of_words = len(word_tokenize(s))\n",
    "                tf_idf_s = float(sm) / float(no_of_words) if no_of_words > 0 else 0\n",
    "                tf_idf_sntnc[ac_s] = tf_idf_s\n",
    "                std_list.append(tf_idf_s)\n",
    "\n",
    "            # STD and final score\n",
    "            sd = np.std(std_list)\n",
    "            for i in range(len(norm_sents)):\n",
    "                s = norm_sents[i]\n",
    "                ac_s = sntncs[i]\n",
    "                ne_list = get_continuous_chunks(sntncs[i])\n",
    "                e = float(len(ne_list)) / float(len(word_tokenize(s)) or 1)\n",
    "                d = 1 if any(char.isdigit() for char in s) else 0\n",
    "                words = word_tokenize(s)\n",
    "                bag = []\n",
    "                for wd in words:\n",
    "                    wd = re.sub(r'[\\[\\]\\(\\)\\{\\}]', '', wd)\n",
    "                    r = re.compile(wd + \".*\")\n",
    "                    newlist = list(filter(r.match, legal_words))\n",
    "                    for item in newlist:\n",
    "                        if item in s:\n",
    "                            bag.extend(item.split(\" \"))\n",
    "                myset = set(bag)\n",
    "                g = float(len(myset)) / float(len(words) or 1)\n",
    "                tf_idf_sntnc[ac_s] += sd * (0.2 * d + 0.3 * e + 1.5 * g)\n",
    "\n",
    "            # ðŸŸ¡ Pick Top N Sentences for Summary\n",
    "            top_n = 40\n",
    "            sorted_x = sorted(tf_idf_sntnc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            top_sentences = set([pair[0] for pair in sorted_x[:top_n]])\n",
    "\n",
    "            # âœ… Preserve original order\n",
    "            summary = \" \".join([s for s in sntncs if s in top_sentences])\n",
    "\n",
    "            # Write Summary\n",
    "            file_nm = os.path.join(output_path, f)\n",
    "            with open(file_nm, \"w\", encoding='utf-8') as w_f:\n",
    "                w_f.write(summary)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {f}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca9a61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    sys.stdout.flush()\n",
    "    read_legal_dict()\n",
    "    cal_df()\n",
    "   \n",
    "    cal_total_doc()\n",
    "    cal_tf_Idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
